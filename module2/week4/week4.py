# -*- coding: utf-8 -*-
"""week4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CPRzq5JQdihIsLEdHTTnpJm_Q4K0Q4e2
"""

import numpy as np

def compute_mean(X):
    return np.mean(X, axis=0)
def compute_mean_naive(X):
  return np.sum(X) / len(X)

X = [2,0,2,2,7,4,-2,5,-1,-1]

print("Mean", compute_mean(X))
print("Naive Mean", compute_mean_naive(X))

def compute_median(X):
    return np.median(X)
def compute_median_basic(X):
    n = len(X)
    X.sort()
    if n % 2 == 1:
        return X[n // 2]
    else:
        return (X[n // 2 - 1] + X[n // 2]) / 2
X = [1, 5, 4, 4, 9, 13]
print("Median", compute_median(X))
print("Naive Median", compute_median_basic(X))

def compute_std(X):
    return np.std(X)
def compute_std_basix(X):
  mean = compute_mean(X)
  variance = sum((x - mean) ** 2 for x in X) / len(X)
  return variance ** 0.5
X = [171, 176, 155, 167, 169, 182]
print("Std", compute_std(X))
print("Naive Std", compute_std_basix(X))

def compute_correlation_cofficient(X, Y):
    return np.corrcoef(X, Y)[0, 1]
def compute_correlation_cofficient_basic(X, Y):
  N = len(X)
  numerator = N * sum(X * Y for X, Y in zip(X, Y)) - (np.sum(X) * np.sum(Y))
  denominator  = np.sqrt(N * sum(X ** 2) - np.sum(X) ** 2) * np.sqrt(N * sum(Y ** 2) - np.sum(Y) ** 2)
  return numerator / denominator
X = np.asarray ([ -2 , -5 , -11 , 6 , 4 , 15 , 9])
Y = np.asarray ([4 , 25 , 121 , 36 , 16 , 225 , 81])
print("Correlation Cofficient", compute_correlation_cofficient(X, Y))
print("Correlation Cofficient naive",  compute_correlation_cofficient_basic(X, Y))

! gdown 1iA0WmVfW88HyJvTBSQDI5vesf-pgKabq

# Download dataset : ! gdown 1 iA0WmVfW88HyJvTBSQDI5vesf - pgKabq
import pandas as pd

data =pd.read_csv ("advertising.csv")

def correlation (x , y ) :
# Your code here #
  return compute_correlation_cofficient_basic (x , y )

# Example usage :
x = data ["TV"]
y = data ["Radio"]
corr_xy = correlation (x , y )
print ( f" Correlation between TV and Sales : { round ( corr_xy , 2)}")

def correlation (x , y ) :
# Your code here #
  return compute_correlation_cofficient_basic(x,y)

features = ["TV", "Radio", "Newspaper"]

for feature_1 in features :
  for feature_2 in features :
    correlation_value = correlation ( data [ feature_1 ] , data [ feature_2 ])
    print ( f" Correlation between { feature_1 } and { feature_2 }: { round (correlation_value , 2)}")

x = data ["Radio"]
y = data ["TV"]

result = np.corrcoef(x, y)
print ( result )

data = pd.read_csv ("advertising.csv")
data.corr()

import matplotlib.pyplot as plt
import seaborn as sns
datacor = data.corr()
sns.heatmap(datacor, annot=True,fmt=".2f", linewidth=.5)

! gdown 1jh2p2DlaWsDo_vEWIcTrNh3mUuXd-cw6

import pandas as pd
import numpy as np
from sklearn . metrics . pairwise import cosine_similarity
from sklearn . feature_extraction . text import TfidfVectorizer

vi_data_df = pd . read_csv ("./vi_text_retrieval.csv")
context = vi_data_df ["text"]
context = [ doc.lower()for doc in context ]

tfidf_vectorizer = TfidfVectorizer ()
context_embedded = tfidf_vectorizer.fit_transform(context)
context_embedded . toarray () [7][0]

def tfidf_search ( question , tfidf_vectorizer , top_d =5) :
 # lowercasing before encoding
  query_embedded = tfidf_vectorizer.transform([question.lower()])
  cosine_scores = cosine_similarity(context_embedded, query_embedded).reshape((-1,))
 # Get top k cosine score and index its
  results = []
  for idx in cosine_scores.argsort()[-top_d:][:: -1]:
    doc_score = {
            "id": idx ,
            "cosine_score": cosine_scores [ idx ]
      }
    results.append ( doc_score )
  return results

question = vi_data_df.iloc[0]["question"]
results = tfidf_search ( question , tfidf_vectorizer , top_d =5)
print(question)

results [0]["id"]

def corr_search ( question , tfidf_vectorizer , top_d =5) :
  # lowercasing before encoding
  query_embedded = tfidf_vectorizer.transform([question.lower()])
  corr_scores = np.corrcoef(
      query_embedded.toarray()[0],
      context_embedded.toarray()
  )
  corr_scores = corr_scores [0][1:]
  # Get top k correlation score and index its
  results = []
  for idx in corr_scores.argsort()[-top_d:][:: -1]:
    doc_score = {
            "id": idx ,
            "corr_score": corr_scores[idx]
      }
    results.append ( doc_score )

  return results

question = vi_data_df.iloc[0]["question"]
results = corr_search ( question , tfidf_vectorizer , top_d =5)
context[results[0]["id"]]

print(question)

